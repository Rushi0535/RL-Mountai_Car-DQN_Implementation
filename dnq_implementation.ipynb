{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Mountain Car \n",
    "\n",
    "- The objective is to drive an underpowered car up a steep mountain road to reach the goal at the top of the hill. The challenge arises from the car’s engine being too weak to climb the hill directly. Therefore, the car must learn to leverage potential energy by building momentum from swinging back and forth between the two hills.\n",
    "\n",
    "### State Space\n",
    "The state of the environment is typically described by two variables:\n",
    "\n",
    "- **Position (x):** The horizontal position of the car along the track, which varies between specific limits, like **-1.2 to 0.6**.\n",
    "- **Velocity (v):** The speed of the car, which also has limits, such as between -0.07 to 0.07.\n",
    "\n",
    "### Action Space\n",
    "The actions that can be taken at any state are typically discrete:\n",
    "\n",
    "- **Accelerate to the Right (0):** Trying to increase the velocity towards the right.\n",
    "- **No Acceleration (1):** No change in the velocity.\n",
    "- **Accelerate to the Left (2):** Trying to increase the velocity towards the left.\n",
    "\n",
    "### Reward Structure\n",
    "The reward structure is simple and is designed to encourage the car to reach the goal as quickly as possible:\n",
    "\n",
    "- **-1 for each time step:** This reward is given until the car reaches the target. The goal is to **minimize the total number of steps taken**, thereby **maximizing the total reward** (or minimizing penalty).\n",
    "\n",
    "### Dynamics of the Game\n",
    "\n",
    "- The dynamics of the game are governed by the **physics of the car’s motion**. The car's **velocity and position** at each **timestep** depend on its **previous velocity**, its **current action**, and **gravitational pull**. \n",
    "\n",
    "- To succeed, the agent must **learn to balance the gravitational pull and the momentum required to reach the peak.**\n",
    "\n",
    "### Link Between Rewards and Action Steps\n",
    "- **Immediate Feedback:** The agent receives a reward (or penalty) after each action, providing immediate feedback about the quality of its decision.\n",
    "\n",
    "- **Cumulative Goal:** The agent receives a negative reward (like -1) for every time step until the goal is reached. This setup encourages the agent to reach the goal in the fewest possible steps since each additional step incurs further penalty.\n",
    "\n",
    "### Game Configrations\n",
    "\n",
    "- **Force** is the action clipped to the range [-1,1] and **Power** is a constant **0.0015**.\n",
    "\n",
    "- **Reward :** A negative reward of **-0.1 * action2** is received at each timestep to penalise for taking actions of large magnitude. If the mountain car reaches the goal then a **positive reward of +100** is added to the negative reward for that timestep.\n",
    "\n",
    "- **Starting State :** The position of the car is assigned a uniform random value in **[-0.6 , -0.4]**. The starting **velocity** of the car is always assigned to **0**.\n",
    "\n",
    "- **Episode End** : The episode ends if either of the following happens:\n",
    "\n",
    "    - 1. **Termination:** The position of the car is greater than or equal to **0.45**. (the goal position on top of the right hill)\n",
    "\n",
    "    - 2. **Truncation:** The length of the episode is **999**.\n",
    "\n",
    "### DQN\n",
    "Deep Q-Networks (DQNs) are an advancement in the field of reinforcement learning that blend traditional Q-learning with deep neural networks. \n",
    "\n",
    "#### WHY TO CHOOSE DQN?\n",
    "- 1. **Function Approximation:** DQN uses deep neural networks to approximate the Q-function, which is the expected rewards for taking an action in a given state. This is useful in environments with a large number of states and actions where traditional methods that require a table for each state-action pair (like Q-learning) become impractical due to memory and computation limitations.\n",
    "\n",
    "- 2. **Handling High-Dimensional Spaces:** DQN is effective in environments with high-dimensional input spaces, such as raw pixel data from video games. The deep learning aspect helps in extracting features and making sense of complex inputs without manual feature engineering. \n",
    "\n",
    "# TRAIN DQN\n",
    "\n",
    "1. **Experience Replay:** Experiences (or transitions), which are tuples of (state, action, reward, next_state), are stored in a replay buffer. \n",
    "\n",
    "    - Collecting Data: As the agent interacts with the environment, each experience (s, a, r, s') is stored in the Replay Buffer. The buffer has a maximum capacity, so when it's full, older experiences are discarded to make room for new ones.\n",
    "\n",
    "    - Batch Learning: During training, instead of updating the network with the latest experience only, the agent samples a mini-batch of experiences randomly from the Replay Buffer. This batch is used to perform a training update on the Primary Network.\n",
    "\n",
    "    - Updating the Primary Network: For each sampled experience, the Primary Network calculates the predicted Q-value for the action taken in the given state. Simultaneously, the Target Network provides the Q-values for the next state, which are used to calculate the target Q-value for training. The Primary Network’s weights are then adjusted to reduce the difference between its predictions and these target Q-values, using a loss function (typically mean squared error).\n",
    "\n",
    "2. **Networks:** DQN employs two networks \n",
    "\n",
    "    - **Primary network:**  \n",
    "        - Function: The Primary Network is the main neural network that actively learns and updates its weights through training. It directly interacts with the environment, deciding which action to take in each state based on the predicted Q-values it generates.\n",
    "        - Updates: As the agent explores the environment and receives feedback (rewards), this network's weights are updated continuously to better predict the Q-values for each action given a state.\n",
    "\n",
    "    - **Target network**\n",
    "        - Function: The Target Network has the same architecture as the Primary Network, but its purpose is different. It helps provide stable target Q-values during the training process. Instead of actively learning, it serves as a somewhat static benchmark against which the Primary Network's predictions are compared.\n",
    "        - Updates: The Target Network's weights are not updated as frequently as those of the Primary Network. Instead, its weights are updated periodically (e.g., every few thousand steps) by directly copying the weights from the Primary Network. This periodic update helps prevent the learning process from becoming unstable.\n",
    "    \n",
    "    - The primary network is updated frequently, while the target network's weights are updated less frequently (every few thousand steps) to provide stable targets during learning.The target network helps in stabilizing the learning updates. If only one network was used, the continuously updating Q-values could lead to significant oscillations or divergence in the learning process.\n",
    "\n",
    "3. **Loss Function:** The loss function used is typically the mean squared error between the predicted Q-value and the target Q-value.\n",
    "\n",
    "\n",
    "\n",
    "### DQN IMPLEMENTATION \n",
    "\n",
    "**Environment Setup:** OpenAI Gym, which offers a pre-built Mountain Car environment.\n",
    "\n",
    "### Algorithm Summary\n",
    "- 1. Initialize the replay buffer.\n",
    "- 2. Initialize the primary and target networks with random weights.\n",
    "- 3. For each episode:\n",
    "    - For each time step:\n",
    "        - Select an action using an ε-greedy policy (to balance exploration and exploitation).\n",
    "        - Execute the action and observe the reward and next state.\n",
    "        - Store the transition in the replay buffer.\n",
    "        - Sample a random batch of transitions from the replay buffer.\n",
    "        - Compute the loss between predicted Q-values and target Q-values.\n",
    "        - Perform a gradient descent step to update the network's weights.\n",
    "        - Every fixed number of steps, update the target network's weights to match the       primary network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Neural Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, buffer_capacity=10000, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, target_update=10, lr=0.001):\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update = target_update\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.primary_network = DQN(env.observation_space.shape[0], env.action_space.n).to(self.device)\n",
    "        self.target_network = DQN(env.observation_space.shape[0], env.action_space.n).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.primary_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.target_network.load_state_dict(self.primary_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "    def select_action(self, state, epsilon=0.0):\n",
    "        if random.random() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.primary_network(state)\n",
    "        return q_values.cpu().numpy().argmax()\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        q_values = self.primary_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action = self.select_action(state, self.epsilon)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                self.replay_buffer.push(state, action, reward, next_state, terminated or truncated)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                self.update()\n",
    "\n",
    "                if terminated:\n",
    "                    print(f\"Episode {episode}: Goal reached in {steps} steps with total reward {total_reward}\")\n",
    "                    break\n",
    "\n",
    "            if not terminated and truncated:\n",
    "                print(f\"Episode {episode}: Truncated after {steps} steps with total reward {total_reward}\")\n",
    "            \n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            if episode % self.target_update == 0:\n",
    "                self.target_network.load_state_dict(self.primary_network.state_dict())\n",
    "            \n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Steps: {steps}, Epsilon: {self.epsilon:.2f}\")\n",
    "\n",
    "    def test(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                if terminated:\n",
    "                    print(f\"Test Episode {episode}: Goal reached in {steps} steps with total reward {total_reward}\")\n",
    "                    break\n",
    "\n",
    "            if not terminated and truncated:\n",
    "                print(f\"Test Episode {episode}: Truncated after {steps} steps with total reward {total_reward}\")\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.primary_network.state_dict(), path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.primary_network.load_state_dict(torch.load(path))\n",
    "        self.primary_network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Episode 0: Goal reached in 109566 steps with total reward -109566.0\n",
      "Episode 0, Total Reward: -109566.0, Steps: 109566, Epsilon: 0.99\n",
      "Episode 1: Goal reached in 84809 steps with total reward -84809.0\n",
      "Episode 1, Total Reward: -84809.0, Steps: 84809, Epsilon: 0.99\n",
      "Episode 2: Goal reached in 34541 steps with total reward -34541.0\n",
      "Episode 2, Total Reward: -34541.0, Steps: 34541, Epsilon: 0.99\n",
      "Episode 3: Goal reached in 24659 steps with total reward -24659.0\n",
      "Episode 3, Total Reward: -24659.0, Steps: 24659, Epsilon: 0.98\n",
      "Episode 4: Goal reached in 30118 steps with total reward -30118.0\n",
      "Episode 4, Total Reward: -30118.0, Steps: 30118, Epsilon: 0.98\n",
      "Episode 5: Goal reached in 61457 steps with total reward -61457.0\n",
      "Episode 5, Total Reward: -61457.0, Steps: 61457, Epsilon: 0.97\n",
      "Episode 6: Goal reached in 11918 steps with total reward -11918.0\n",
      "Episode 6, Total Reward: -11918.0, Steps: 11918, Epsilon: 0.97\n",
      "Episode 7: Goal reached in 6327 steps with total reward -6327.0\n",
      "Episode 7, Total Reward: -6327.0, Steps: 6327, Epsilon: 0.96\n",
      "Episode 8: Goal reached in 17365 steps with total reward -17365.0\n",
      "Episode 8, Total Reward: -17365.0, Steps: 17365, Epsilon: 0.96\n",
      "Episode 9: Goal reached in 3810 steps with total reward -3810.0\n",
      "Episode 9, Total Reward: -3810.0, Steps: 3810, Epsilon: 0.95\n",
      "Episode 10: Goal reached in 29475 steps with total reward -29475.0\n",
      "Episode 10, Total Reward: -29475.0, Steps: 29475, Epsilon: 0.95\n",
      "Episode 11: Goal reached in 35148 steps with total reward -35148.0\n",
      "Episode 11, Total Reward: -35148.0, Steps: 35148, Epsilon: 0.94\n",
      "Episode 12: Goal reached in 7016 steps with total reward -7016.0\n",
      "Episode 12, Total Reward: -7016.0, Steps: 7016, Epsilon: 0.94\n",
      "Episode 13: Goal reached in 35649 steps with total reward -35649.0\n",
      "Episode 13, Total Reward: -35649.0, Steps: 35649, Epsilon: 0.93\n",
      "Episode 14: Goal reached in 10290 steps with total reward -10290.0\n",
      "Episode 14, Total Reward: -10290.0, Steps: 10290, Epsilon: 0.93\n",
      "Episode 15: Goal reached in 13000 steps with total reward -13000.0\n",
      "Episode 15, Total Reward: -13000.0, Steps: 13000, Epsilon: 0.92\n",
      "Episode 16: Goal reached in 15810 steps with total reward -15810.0\n",
      "Episode 16, Total Reward: -15810.0, Steps: 15810, Epsilon: 0.92\n",
      "Episode 17: Goal reached in 547496 steps with total reward -547496.0\n",
      "Episode 17, Total Reward: -547496.0, Steps: 547496, Epsilon: 0.91\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m agent\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 101\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m     99\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    100\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Goal reached in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps with total reward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 86\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     85\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rushi\\miniconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    372\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 373\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rushi\\miniconda3\\Lib\\site-packages\\torch\\autograd\\profiler.py:622\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 622\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    624\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\rushi\\miniconda3\\Lib\\site-packages\\torch\\_ops.py:513\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "agent = DQNAgent(env)\n",
    "print(\"Training...\")\n",
    "agent.train(500)\n",
    "agent.save_model(\"dqn_model.pth\")\n",
    "print(\"Testing...\")\n",
    "agent.load_model(\"dqn_model.pth\")\n",
    "agent.test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -677.0983350800102, Steps: 23312, Epsilon: 0.99\n",
      "Episode 1, Total Reward: -2289.4405299698055, Steps: 72117, Epsilon: 0.99\n",
      "Episode 2, Total Reward: -533.1453041588754, Steps: 19133, Epsilon: 0.99\n",
      "Episode 3, Total Reward: 15.796586472902149, Steps: 2557, Epsilon: 0.98\n",
      "Episode 4, Total Reward: -872.0823157868286, Steps: 29682, Epsilon: 0.98\n",
      "Episode 5, Total Reward: -2439.001657126396, Steps: 77285, Epsilon: 0.97\n",
      "Episode 6, Total Reward: -578.6790567485673, Steps: 18063, Epsilon: 0.97\n",
      "Episode 7, Total Reward: -270.0510298789522, Steps: 11577, Epsilon: 0.96\n",
      "Episode 8, Total Reward: -2425.1689193061766, Steps: 78237, Epsilon: 0.96\n",
      "Episode 9, Total Reward: -157.87936639274682, Steps: 8066, Epsilon: 0.95\n",
      "Episode 10, Total Reward: -685.9014909146756, Steps: 24516, Epsilon: 0.95\n",
      "Episode 11, Total Reward: -31.12261644414386, Steps: 4004, Epsilon: 0.94\n",
      "Episode 12, Total Reward: -99.40518521013021, Steps: 6113, Epsilon: 0.94\n",
      "Episode 13, Total Reward: -358.0181566752662, Steps: 14000, Epsilon: 0.93\n",
      "Episode 14, Total Reward: -1042.8439961928402, Steps: 36001, Epsilon: 0.93\n",
      "Episode 15, Total Reward: -512.2565394744665, Steps: 18340, Epsilon: 0.92\n",
      "Episode 16, Total Reward: -2016.0398157976165, Steps: 66089, Epsilon: 0.92\n",
      "Episode 17, Total Reward: -1504.0010475767824, Steps: 51169, Epsilon: 0.91\n",
      "Episode 18, Total Reward: -835.6528371728803, Steps: 29931, Epsilon: 0.91\n",
      "Episode 19, Total Reward: -2924.989239639066, Steps: 97069, Epsilon: 0.90\n",
      "Episode 20, Total Reward: -198.41472020240198, Steps: 9495, Epsilon: 0.90\n",
      "Episode 21, Total Reward: -1691.9995054420149, Steps: 55190, Epsilon: 0.90\n",
      "Episode 22, Total Reward: -1698.4049235069624, Steps: 57668, Epsilon: 0.89\n",
      "Episode 23, Total Reward: -378.20604427397734, Steps: 15411, Epsilon: 0.89\n",
      "Episode 24, Total Reward: -1265.3639461751573, Steps: 44386, Epsilon: 0.88\n",
      "Episode 25, Total Reward: -457.0412497758332, Steps: 18326, Epsilon: 0.88\n",
      "Episode 26, Total Reward: -1535.1245854771814, Steps: 50293, Epsilon: 0.87\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Neural Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, buffer_capacity=10000, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, target_update=10, lr=0.001):\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.target_update = target_update\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.primary_network = DQN(env.observation_space.shape[0], env.action_space.shape[0]).to(self.device)\n",
    "        self.target_network = DQN(env.observation_space.shape[0], env.action_space.shape[0]).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.primary_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.target_network.load_state_dict(self.primary_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.primary_network(state)\n",
    "        return q_values.cpu().numpy()[0]\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        q_values = self.primary_network(states)\n",
    "        next_q_values = self.target_network(next_states)\n",
    "        max_next_q_values = next_q_values.max(1)[0]\n",
    "        target_q_values = rewards + (self.gamma * max_next_q_values * (1 - dones))\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values, target_q_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                self.update()\n",
    "            \n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            if episode % self.target_update == 0:\n",
    "                self.target_network.load_state_dict(self.primary_network.state_dict())\n",
    "            \n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}, Steps: {steps}, Epsilon: {self.epsilon:.2f}\")\n",
    "\n",
    "# Example Usage\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "agent = DQNAgent(env)\n",
    "agent.train(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Action Space\n",
    "In the discrete version of the Mountain Car game:\n",
    "\n",
    "- **Action Space:** The action space consists of a small, finite set of possible actions. Typically, these are:\n",
    "    - 0: Accelerate to the Right\n",
    "    - 1: No Acceleration\n",
    "    - 2: Accelerate to the Left\n",
    "- **Control:** The agent selects one of these discrete actions at each time step.\n",
    "- **Implementation:** This simplification makes it easier to implement traditional RL algorithms like Q-learning or SARSA, which are well-suited for discrete spaces.\n",
    "\n",
    "### Continuous Action Space\n",
    "In the continuous version, called Mountain Car Continuous:\n",
    "\n",
    "- **Action Space:** The action space is continuous, typically represented by a real number within a range. For instance, the action might be a single real value indicating the amount of force applied in either direction.\n",
    "    - The action can range, for example, from -1 (full power to the left) to +1 (full power to the right).\n",
    "- **Control:** The agent can choose any value within this range, allowing for finer control over the car's acceleration.\n",
    "- **Implementation:** This requires different types of RL algorithms that can handle continuous action spaces, such as Deep Deterministic Policy Gradient (DDPG), Proximal Policy Optimization (PPO), DNQ or other policy gradient methods that work directly with continuous outputs.\n",
    "\n",
    "### Key Differences\n",
    "- **Action Granularity:** The main difference is in the granularity of the actions that can be taken. Continuous action spaces allow for more precise and varied actions, whereas discrete action spaces are limited to a few predefined options.\n",
    "- **Complexity:** Continuous action spaces generally increase the complexity of the problem, as the agent needs to learn to choose optimally from an infinite set of possible actions.\n",
    "- **Algorithms:** Continuous spaces often require more sophisticated RL algorithms that are specifically designed to handle continuous domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
